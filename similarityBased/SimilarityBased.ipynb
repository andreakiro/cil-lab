{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d982d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0df6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MOVIES = 1000\n",
    "N_USERS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926e151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_matrix():\n",
    "    '''\n",
    "    Get the input matrix\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    (data, W): (np.array(N_USERS, N_MOVIES), np.array(N_USERS, N_MOVIES))\n",
    "        The input array with the true ratings and Nan where no ratings where given and the \n",
    "        array containing 1 where the entries are given and 0 otherwise.\n",
    "    '''\n",
    "\n",
    "    data_pd = pd.read_csv('../data/data_train.csv') \n",
    "\n",
    "    # get users, movies\n",
    "    users, movies = [np.squeeze(arr) \n",
    "                    for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
    "    # get predictions\n",
    "    predictions = data_pd.Prediction.values\n",
    "\n",
    "    # create data matrix\n",
    "    data = np.full((N_USERS, N_MOVIES), np.nan)\n",
    "    W = np.full((N_USERS, N_MOVIES), 0)\n",
    "\n",
    "    # populate data matrix\n",
    "    for user, movie, pred in zip(users, movies, predictions): \n",
    "        data[user][movie] = pred\n",
    "        W[user][movie] = 1 if not math.isnan(pred) else 0\n",
    "    \n",
    "    return (data, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f3283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, W = get_input_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0042381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8797ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(X, user=True):\n",
    "    '''\n",
    "    Compute the cosine similarity between every pair of users or items\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings.\n",
    "        \n",
    "    user : bool, default True\n",
    "           a boolean which says whether we want to compute the cosine similarity between every users or \n",
    "           between every items.\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    similarity: np.array(N_USERS, N_USERS) if user = True else np.array(N_MOVIES, N_MOVIES):\n",
    "                The similarity score (between -1 and 1 if X has negative values, where -1 means two vectors \n",
    "                going in the opposite direction or between 0 and 1 if we only have positive values where 0 means\n",
    "                orthogonal vectors) for each user-user or item-item pair. The returned matrix is therefore \n",
    "                symmetric.\n",
    "    '''\n",
    "    X = np.nan_to_num(X) # Replace Nan by 0 (the dot product will hence be 0, what we want)\n",
    "    \n",
    "    if not user:\n",
    "        X = X.T\n",
    "    \n",
    "    similarity = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i,user in enumerate(X):\n",
    "        similarity[i, :] = (X@user)/(np.linalg.norm(X, axis=1)*np.linalg.norm(user))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20cd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_correlation_coefficient(X, user=True, statistic_to_use=\"mean\"):\n",
    "    '''\n",
    "    Compute the pearson correlation coefficient between every pair of users or items\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings.\n",
    "        \n",
    "    user : bool, default True\n",
    "           a boolean which says whether we want to compute the cosine similarity between every users or \n",
    "           between every items.\n",
    "    statistic_to_use: String, either 'mean' or 'median', default 'mean'\n",
    "                      The method use to center the data points.\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    similarity: np.array(N_USERS, N_USERS) if user = True else np.array(N_MOVIES, N_MOVIES):\n",
    "                The similarity score (between -1 and 1) for each user-user or item-item pair.\n",
    "                The returned matrix is therefore symmetric.\n",
    "    '''\n",
    "    if not user:\n",
    "        X = X.T\n",
    "    \n",
    "    if statistic_to_use == \"mean\":\n",
    "        statistic = np.nanmean(X, axis=1)\n",
    "    elif statistic_to_use == \"median\":\n",
    "        statistic = np.nanmedian(X, axis=1)\n",
    "    else:\n",
    "        raise ValueError(f\"{statistic_to_use} is not a valid statistic! Should be 'mean' or 'median'\")\n",
    "    \n",
    "    centered_X = X-statistic.reshape(-1,1)\n",
    "    \n",
    "    return compute_cosine_similarity(centered_X, user=True) # Always True since we have already taken the transpose in this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e66a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SigRA(X, W, user=True):\n",
    "    '''\n",
    "    Compute the SiGra (https://ieeexplore.ieee.org/document/8250351) similarity between every pair of users or \n",
    "    items. Note that this method already uses weighting and hence should not be followed by the \n",
    "    similarity_weighting function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings.\n",
    "    \n",
    "    W : np.array(N_USERS, N_MOVIES):\n",
    "        The mask containing 1 for rated items and 0 for unrated items.\n",
    "        \n",
    "    user : bool, default True\n",
    "           a boolean which says whether we want to compute the cosine similarity between every users or \n",
    "           between every items.\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    similarity: np.array(N_USERS, N_USERS) if user = True else np.array(N_MOVIES, N_MOVIES):\n",
    "                The similarity score for each user-user or item-item pair.\n",
    "                The returned matrix is therefore symmetric.\n",
    "    '''    \n",
    "    if not user:\n",
    "        X = X.T\n",
    "        W = W.T\n",
    "    \n",
    "    similarity = np.zeros((X.shape[0], X.shape[0]))\n",
    "\n",
    "    number_ratings = np.sum(W, axis=1)\n",
    "    \n",
    "    for i, (uw, ux) in enumerate(zip(W, X)):\n",
    "        for j, (vw, vx) in enumerate(zip(W, X)):\n",
    "            common_ratings = np.logical_and(uw, vw)\n",
    "            number_common_ratings = np.sum(common_ratings)\n",
    "            if number_common_ratings == 0:\n",
    "                similarity[i, j] = 0\n",
    "            else:\n",
    "                ratios_sum = np.sum(np.minimum(ux[common_ratings], vx[common_ratings])/np.maximum(ux[common_ratings], vx[common_ratings]))\n",
    "                weight = 1.0/(1+np.exp(-(number_ratings[i] + number_ratings[j])/(2*number_common_ratings))) #Why number_common_ratings in the denominator? Would make more sense to inverse numerator and denominator, but like that in the paper\n",
    "                similarity[i, j] = weight*ratios_sum/number_common_ratings\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a35ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_weighting(similarity, W, method=\"weighting\", threshold=10):\n",
    "    '''\n",
    "    Weight the similarity matrix based on the number of ratings of each entry. Without weighting, users having \n",
    "    just few entries are often considered as closer, which this method tries to prevent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    similarity : np.array(N_USERS, N_USERS) or np.array(N_MOVIES, N_MOVIES)\n",
    "                 The matrix with similarity between users or movies\n",
    "        \n",
    "    W : np.array(N_USERS, N_MOVIES):\n",
    "        The mask containing 1 for rated items and 0 for unrated items.\n",
    "        \n",
    "    method: String, either 'weighting', 'significance' or 'sigmoid', default 'weighting'\n",
    "                      The method use to weight the similarity.\n",
    "                      Weighting weights all entries based on the number of common rated items and number of rated items.\n",
    "                      Significance only reduce importance when number of common rated items is below the threshold.\n",
    "                      Sigmoid reduce weight when users have only few common rated items. It keeps most of the similarity\n",
    "                      measure almost untouched and hence is the softest weighting method.\n",
    "    \n",
    "    threshold: int, default 10\n",
    "               Only used if method is 'significance'. Minimum number of common rated items needed to not have\n",
    "               a decrease in importance. Needs to be adapted depending on the number of common users/items. For\n",
    "               user-based similarity, should be around 7, for item-based similarity, around 70. \n",
    "               TODO: read paper to have more knowledge about what a good threshold is.\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    weighted_similarity: np.array(N_USERS, N_USERS) or np.array(N_MOVIES, N_MOVIES) depending of shape of similarity:\n",
    "                The weighted similarity score (between -1 and 1) for each user-user or item-item pair.\n",
    "                The returned matrix is therefore symmetric.\n",
    "    '''\n",
    "    assert (similarity.shape[0] == W.shape[0] or similarity.shape[0] == W.shape[1]) and similarity.shape[0] == similarity.shape[1]\n",
    "    \n",
    "    weighted_similarity = np.zeros_like(similarity)\n",
    "    if similarity.shape[0] != W.shape[0]:\n",
    "        W=W.T # We were using the items and not the users for the similarity\n",
    "    \n",
    "    number_ratings = np.sum(W, axis=1)\n",
    "    \n",
    "    for i, u in enumerate(W):\n",
    "        for j, v in enumerate(W):\n",
    "            number_common_ratings = np.sum(np.logical_and(u, v))\n",
    "            \n",
    "            if method == \"weighting\":\n",
    "                weight = 2*number_common_ratings/(number_ratings[i] + number_ratings[j]) if (number_ratings[i] + number_ratings[j]) != 0 else 0\n",
    "            elif method == \"significance\":\n",
    "                weight = np.minimum(number_common_ratings, threshold)/threshold\n",
    "            elif method == \"sigmoid\":\n",
    "                weight = 1.0/(1+np.exp(-number_common_ratings/2))\n",
    "            else:\n",
    "                raise ValueError(f\"{method} is not a valid method! Should be 'weighting', 'significance' or 'sigmoid'\")\n",
    "\n",
    "            weighted_similarity[i, j] = weight*similarity[i, j]\n",
    "    \n",
    "    return weighted_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a33d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = compute_cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc43d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_items = compute_cosine_similarity(X, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc8bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_users_pearson=compute_pearson_correlation_coefficient(X, statistic_to_use=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c15f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.01367172,  0.09147787, ...,  0.14142136,\n",
       "         0.10314212,  0.05800419],\n",
       "       [-0.01367172,  1.        ,  0.0125066 , ...,  0.03625262,\n",
       "         0.0141013 ,  0.0528678 ],\n",
       "       [ 0.09147787,  0.0125066 ,  1.        , ...,  0.03234231,\n",
       "         0.03774089,  0.15033949],\n",
       "       ...,\n",
       "       [ 0.14142136,  0.03625262,  0.03234231, ...,  1.        ,\n",
       "         0.10939874,  0.0341793 ],\n",
       "       [ 0.10314212,  0.0141013 ,  0.03774089, ...,  0.10939874,\n",
       "         1.        ,  0.08974013],\n",
       "       [ 0.05800419,  0.0528678 ,  0.15033949, ...,  0.0341793 ,\n",
       "         0.08974013,  1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_users_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09c2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_items_pearson=compute_pearson_correlation_coefficient(X, False, statistic_to_use=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f9e935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.02009841,  0.04217509, ...,  0.02842744,\n",
       "         0.0526932 ,  0.03357856],\n",
       "       [-0.02009841,  1.        , -0.01603365, ..., -0.02378553,\n",
       "         0.00502976,  0.02908714],\n",
       "       [ 0.04217509, -0.01603365,  1.        , ...,  0.05456939,\n",
       "         0.04203638, -0.01824104],\n",
       "       ...,\n",
       "       [ 0.02842744, -0.02378553,  0.05456939, ...,  1.        ,\n",
       "         0.08803759,  0.01628506],\n",
       "       [ 0.0526932 ,  0.00502976,  0.04203638, ...,  0.08803759,\n",
       "         1.        ,  0.03081192],\n",
       "       [ 0.03357856,  0.02908714, -0.01824104, ...,  0.01628506,\n",
       "         0.03081192,  1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_items_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b42687",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_similarity_items_pearson = similarity_weighting(similarity_items_pearson, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2cc444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_similarity_users_pearson = similarity_weighting(similarity_users_pearson, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21dd8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_similarity = similarity_weighting(similarity_items, W, method=\"significance\", threshold=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2411482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_similarity = similarity_weighting(similarity_items, W, method=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4216335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similartiry_items_SiGra = compute_SigRA(X, W, user=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff45d5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73105858, 0.72079709, 0.73050547, ..., 0.68383528, 0.72625998,\n",
       "        0.78522803],\n",
       "       [0.72079709, 0.73105858, 0.68952065, ..., 0.67338861, 0.74773731,\n",
       "        0.75743673],\n",
       "       [0.73050547, 0.68952065, 0.73105858, ..., 0.71954167, 0.76983944,\n",
       "        0.69953118],\n",
       "       ...,\n",
       "       [0.68383528, 0.67338861, 0.71954167, ..., 0.73105858, 0.77151787,\n",
       "        0.71935246],\n",
       "       [0.72625998, 0.74773731, 0.76983944, ..., 0.77151787, 0.73105858,\n",
       "        0.75868866],\n",
       "       [0.78522803, 0.75743673, 0.69953118, ..., 0.71935246, 0.75868866,\n",
       "        0.73105858]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similartiry_items_SiGra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8582da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_predict(X, W, similarity, k=10, with_std = False, verbose=True, min_similarity_neighbor=0):\n",
    "    '''\n",
    "    Predict the missing values by a weighted average of the ratings of the k nearest neighbors with a weight \n",
    "    corresponding to their similarity. Take into account the mean value of the user (respectively item). Uses\n",
    "    only item based similarity matrix or user based similarity matrix\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings.\n",
    "    \n",
    "    W : np.array(N_USERS, N_MOVIES):\n",
    "        The mask containing 1 for rated items and 0 for unrated items.\n",
    "    \n",
    "    similarity : np.array(N_USERS, N_USERS) or np.array(N_MOVIES, N_MOVIES)\n",
    "                 The matrix with similarity between users or movies\n",
    "        \n",
    "    k: int, default 10\n",
    "       Number of nearest neighbors\n",
    "    \n",
    "    with_std: bool, default False\n",
    "              If set to true, take into account the std to compute a Z-score when computing the weights\n",
    "\n",
    "    verbose: bool, default True:\n",
    "             If set to True, print update messages\n",
    "    \n",
    "    min_similarity_neighbor: int, default 0:\n",
    "            Minimum value needed for the similarity to be considered as a neighbor. For PCC, should be 0, for \n",
    "            similarities ranging between 0 and 1 should be close to 0.5.\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    predictions: np.array(N_USERS, N_MOVIES)\n",
    "                 The predictions for the missing values\n",
    "    \n",
    "    confidence: np.array(N_USERS, N_MOVIES)\n",
    "                 The confidence based on the similarity of neighbors used to compute it (if neighbors have high\n",
    "                 similarity, will give high confidence).\n",
    "    '''\n",
    "    MIN_POSSIBLE_RATING = 1\n",
    "    MAX_POSSIBLE_RATING = 5\n",
    "    \n",
    "    was_transposed=False\n",
    "    printing_interval=200\n",
    "    \n",
    "    if similarity.shape[0] != W.shape[0]: # We were using the items and not the users for the similarity \n",
    "        was_transposed=True\n",
    "        W=W.T   \n",
    "        X=X.T\n",
    "        printing_interval=30\n",
    "    \n",
    "    predictions = X.copy()\n",
    "    confidence = np.ones_like(predictions)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        \n",
    "        user_mean = np.nanmean(X[i, :])\n",
    "        user_mean = np.nan_to_num(user_mean, nan=(MAX_POSSIBLE_RATING+MIN_POSSIBLE_RATING)/2) # Replace Nan by medium value if a row was full of Nan\n",
    "\n",
    "        if with_std:\n",
    "            number_items_rated = np.sum(W[i, :])\n",
    "            user_std = np.sqrt(np.nansum((X[i, :]-user_mean)**2)/(number_items_rated-1)) if number_items_rated > 1 else 1\n",
    "        \n",
    "        for j in range(X.shape[1]):\n",
    "            if not W[i, j]:\n",
    "                possible_neighbors = np.where(np.logical_and(W[:, j], similarity[i, :]>min_similarity_neighbor))[0]\n",
    "                sorted_possible_neighbors = possible_neighbors[np.flip(np.argsort(similarity[i, possible_neighbors]))]\n",
    "                nearest_neighbors = sorted_possible_neighbors[:k]\n",
    "\n",
    "                if nearest_neighbors.shape[0] == 0:\n",
    "                    predictions[i, j] = user_mean\n",
    "                elif with_std:                                        \n",
    "                    neighbors_number_item_rated = np.sum(W[nearest_neighbors, :], axis=1)\n",
    "                    neighbors_means = np.nanmean(X[nearest_neighbors, :], axis=1)\n",
    "                    neighbors_means = np.nan_to_num(neighbors_means, nan=(MAX_POSSIBLE_RATING+MIN_POSSIBLE_RATING)/2) # Replace Nan by medium value if a row was full of Nan\n",
    "                    neighbors_number_item_rated[neighbors_number_item_rated<=1]=2 #To avoid division by 0 problems, should not happen frequently, set std to 1 later\n",
    "                    \n",
    "                    neighbors_stds = np.sqrt(np.nansum((X[nearest_neighbors, :]-neighbors_means.reshape(-1,1))**2, axis=1)/(neighbors_number_item_rated-1))\n",
    "                    neighbors_stds[neighbors_number_item_rated<=1] = 1 #Set std to 1 if it was the only rating\n",
    "\n",
    "                    predictions[i, j] = user_mean + user_std * np.sum(np.multiply(similarity[i, nearest_neighbors], (X[nearest_neighbors, j]-np.nanmean(X[nearest_neighbors, :], axis=1))/neighbors_stds))/np.sum(similarity[i, nearest_neighbors])\n",
    "                else:  \n",
    "                    predictions[i, j] = user_mean + np.sum(np.multiply(similarity[i, nearest_neighbors], X[nearest_neighbors, j]-np.nanmean(X[nearest_neighbors, :], axis=1)))/np.sum(similarity[i, nearest_neighbors])\n",
    "                \n",
    "                confidence[i, j] = np.sum(similarity[i, nearest_neighbors]) if nearest_neighbors.shape[0] != 0 else 0\n",
    "                \n",
    "                \n",
    "        if verbose and (i==X.shape[0]-1 or (not i%printing_interval and i!=0)):\n",
    "            similarity_type = \"user\" if not was_transposed else \"item\"\n",
    "            print(f\"Done with {similarity_type} {i}/{X.shape[0]}\")\n",
    "    \n",
    "    predictions = np.clip(predictions, MIN_POSSIBLE_RATING, MAX_POSSIBLE_RATING) # Might exceed it, so we clip to correct range\n",
    "    \n",
    "    if was_transposed:\n",
    "        predictions = predictions.T\n",
    "        confidence = confidence.T\n",
    "    \n",
    "    return predictions, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e97d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(X):\n",
    "    '''\n",
    "    Apply min-max normalization to the given matrix, not taking into account the Nan values.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings.\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    X_norm: np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings normalized in range [0, 1].\n",
    "    '''\n",
    "    min_val = np.nanmin(X)\n",
    "    max_val = np.nanmax(X)\n",
    "    return (X-min_val)/(max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0221c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_CSR(users_similarity, items_similarity, X, W, alpha=0.5 , epsilon=0.1, max_iter=10, min_similarity_positive=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Implement the Comprehensive Similarity Reinforcement algorithm, which refine the user similarity using items similarity\n",
    "    and vice versa. See paper at https://dl.acm.org/doi/pdf/10.1145/3062179 for more informations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    users_similarity : np.array(N_USERS, N_USERS)\n",
    "                 The matrix with similarity between users. In paper, they use a weighted PCC.\n",
    "    \n",
    "    items_similarity: np.array(N_MOVIES, N_MOVIES)\n",
    "                 The matrix with similarity between items. In paper, they use a weighted PCC.\n",
    "    \n",
    "    X : np.array(N_USERS, N_MOVIES)\n",
    "        The matrix with ratings.\n",
    "    \n",
    "    W : np.array(N_USERS, N_MOVIES):\n",
    "        The mask containing 1 for rated items and 0 for unrated items.\n",
    "        \n",
    "    alpha: float in range (0,1), default 0.5\n",
    "        Update parameter. If small, the reinforced matrix will change only slightly at each iteration.\n",
    "    \n",
    "    epsilon: float, default 0.1\n",
    "        Threshold used to stop the iterations when the Frobenius norm of the difference between two \n",
    "        iterations is below it for both user and item matrix.\n",
    "\n",
    "    max_iter: int, default 10\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    min_similarity_positive: int, default 0:\n",
    "        Minimum value needed for the similarity to be considered as positive. For PCC, should be 0, for \n",
    "        similarities ranging between 0 and 1 should be close to 0.5.\n",
    "\n",
    "    verbose: bool, default True:\n",
    "        If set to True, print update messages\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    users_reinforced_similarity: np.array(N_USERS, N_USERS)\n",
    "                 The reinforced user similarity matrix\n",
    "                 \n",
    "    items_reinforced_similarity: np.array(N_MOVIES, N_MOVIES)\n",
    "                 The reinforced item similarity matrix\n",
    "    \"\"\"\n",
    "    assert alpha >= 0 and alpha <= 1\n",
    "    users_reinforced_similarity = users_similarity.copy()\n",
    "    items_reinforced_similarity = items_similarity.copy()\n",
    "    \n",
    "    X = min_max_normalization(X)\n",
    "    \n",
    "    for iter_cur in range(max_iter):\n",
    "        last_users_reinforced_similarity = users_reinforced_similarity.copy()\n",
    "        last_items_reinforced_similarity = items_reinforced_similarity.copy()\n",
    "        \n",
    "        #Update users\n",
    "        for user_0 in range(X.shape[0]):\n",
    "            rated_items_user0 = np.where(W[user_0, :])[0]\n",
    "            if rated_items_user0.shape[0]==0: #Should not happen\n",
    "                break\n",
    "\n",
    "            for user_1 in range(user_0+1, X.shape[0]):\n",
    "                rated_items_user1 = np.where(W[user_1, :])[0]\n",
    "                if rated_items_user1.shape[0]==0: #Should not happen\n",
    "                    break\n",
    "                \n",
    "                indices = np.array(list(itertools.product(rated_items_user0, rated_items_user1))) # All pairs of indices, shape (rated_items_user0*rated_items_user1, 2)\n",
    "                pos_neg_indices_users = indices[last_items_reinforced_similarity[indices[:, 0], indices[:, 1]]!=min_similarity_positive]\n",
    "                w_users = 1-2*np.abs(X[user_0, pos_neg_indices_users[:,0]]-X[user_1, pos_neg_indices_users[:,1]])\n",
    "                total_w_users = np.sum(np.abs(w_users))\n",
    "\n",
    "                if total_w_users != 0:\n",
    "                    update_users = np.sum(w_users*last_items_reinforced_similarity[pos_neg_indices_users[:,0], pos_neg_indices_users[:,1]])/total_w_users\n",
    "                    new_sim_users = (1-alpha)*last_users_reinforced_similarity[user_0, user_1] + alpha * update_users\n",
    "                    users_reinforced_similarity[user_0, user_1] = new_sim_users\n",
    "                    users_reinforced_similarity[user_1, user_0] = new_sim_users\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Done with users of iteration {iter_cur+1}/{max_iter}\")\n",
    "        \n",
    "        #Update items\n",
    "        for item_0 in range(X.shape[1]):\n",
    "            rated_users_item0 = np.where(W[:, item_0])[0]\n",
    "            if rated_users_item0.shape[0]==0: #Should not happen\n",
    "                break\n",
    "\n",
    "            for item_1 in range(item_0+1, X.shape[1]):\n",
    "                rated_users_item1 = np.where(W[:,item_1])[0]\n",
    "                if rated_users_item1.shape[0]==0: #Should not happen\n",
    "                    break\n",
    "\n",
    "                indices = np.array(list(itertools.product(rated_users_item0, rated_users_item1))) # All pairs of indices, shape (rated_users_item0*rated_users_item1, 2)\n",
    "                pos_neg_indices_items = indices[users_reinforced_similarity[indices[:, 0], indices[:, 1]]!=min_similarity_positive]\n",
    "                w_items = 1-2*np.abs(X[pos_neg_indices_items[:,0], item_0]-X[pos_neg_indices_items[:,1], item_1])\n",
    "                total_w_items = np.sum(np.abs(w_items))\n",
    "\n",
    "                if total_w_items != 0:\n",
    "                    update_items = np.sum(w_items*users_reinforced_similarity[pos_neg_indices_items[:,0], pos_neg_indices_items[:,1]])/total_w_items\n",
    "                    new_sim = (1-alpha)*last_items_reinforced_similarity[item_0, item_1] + alpha * update_items\n",
    "                    items_reinforced_similarity[item_0, item_1] = new_sim\n",
    "                    items_reinforced_similarity[item_1, item_0] = new_sim\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Done with iteration {iter_cur+1}/{max_iter}\")\n",
    "        \n",
    "        #Check for stopping condition\n",
    "        dU = np.linalg.norm(users_reinforced_similarity-last_users_reinforced_similarity, ord=\"fro\")\n",
    "        dI = np.linalg.norm(items_reinforced_similarity-last_items_reinforced_similarity, ord=\"fro\")\n",
    "        \n",
    "        if dU < epsilon and dI < epsilon:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping due to convergence (difference between two runs smaller than epsilon = {epsilon})\")\n",
    "            break\n",
    "        \n",
    "    return users_reinforced_similarity, items_reinforced_similarity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ab55b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with users of iteration 1/5\n",
      "Done with iteration 1/5\n",
      "Done with users of iteration 2/5\n",
      "Done with iteration 2/5\n",
      "Done with users of iteration 3/5\n",
      "Done with iteration 3/5\n",
      "Done with users of iteration 4/5\n",
      "Done with iteration 4/5\n",
      "Early stopping due to convergence (difference between two runs smaller than epsilon = 0.1)\n"
     ]
    }
   ],
   "source": [
    "n_userrrr = 100\n",
    "n_itemsss = 300\n",
    "users_reinforced_similarity, items_reinforced_similarity = compute_CSR(weighted_similarity_users_pearson[:n_userrrr, :n_userrrr], weighted_similarity_items_pearson[:n_itemsss, :n_itemsss], X[:n_userrrr, :n_itemsss], W[:n_userrrr, :n_itemsss], max_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2a6f2cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00839795, 0.0099411 , ..., 0.0322515 , 0.00354424,\n",
       "        0.01389177],\n",
       "       [0.00839795, 1.        , 0.00898864, ..., 0.00932259, 0.01243429,\n",
       "        0.00497444],\n",
       "       [0.0099411 , 0.00898864, 1.        , ..., 0.00894849, 0.00684133,\n",
       "        0.00844839],\n",
       "       ...,\n",
       "       [0.0322515 , 0.00932259, 0.00894849, ..., 1.        , 0.0123078 ,\n",
       "        0.00723837],\n",
       "       [0.00354424, 0.01243429, 0.00684133, ..., 0.0123078 , 1.        ,\n",
       "        0.00563348],\n",
       "       [0.01389177, 0.00497444, 0.00844839, ..., 0.00723837, 0.00563348,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_reinforced_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e0325ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 299, 299, 299]),\n",
       " array([  1,   4,   5, ..., 284, 288, 293]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "29bf3317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with user 49/50\n",
      "Done with item 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-359-89dde059edf0>:58: RuntimeWarning: Mean of empty slice\n",
      "  user_mean = np.nanmean(X[i, :])\n"
     ]
    }
   ],
   "source": [
    "n_userrrr = 50\n",
    "n_itemsss = 30\n",
    "users_pred, users_confidence = weighted_average_predict(X[:n_userrrr, :n_itemsss], W[:n_userrrr, :n_itemsss], similarity_users_pearson[:n_userrrr, :n_userrrr])\n",
    "items_pred, items_confidence = weighted_average_predict(X[:n_userrrr, :n_itemsss], W[:n_userrrr, :n_itemsss], similarity_items_pearson[:n_itemsss, :n_itemsss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "fef29e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_users_and_items(users_pred, items_pred, users_confidence, items_confidence, user_weight=0.5):\n",
    "    \"\"\"\n",
    "    Compute the final prediction using both user and items predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    users_pred : np.array(N_USERS, N_MOVIES)\n",
    "                 The matrix of predictions based on the neighbors of the users\n",
    "    \n",
    "    items_pred: np.array(N_USERS, N_MOVIES)\n",
    "                 The matrix of predictions based on the neighbors of the items\n",
    "    \n",
    "    users_confidence : np.array(N_USERS, N_MOVIES)\n",
    "                 The matrix of confidence based on the neighbors of the users\n",
    "    \n",
    "    items_confidence: np.array(N_USERS, N_MOVIES)\n",
    "                 The matrix of confidence based on the neighbors of the items\n",
    "        \n",
    "    user_weight: float in range [0,1], default 0.5\n",
    "        Used to add manually more weight to user or items prediction\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    final_predictions: np.array(N_USERS, N_MOVIES)\n",
    "                 The Final prediction\n",
    "    \"\"\"\n",
    "    assert user_weight>=0 and user_weight<=1\n",
    "    \n",
    "    no_predictions = np.where((user_weight*users_confidence + (1-user_weight)*items_confidence)==0) #Should not happen\n",
    "    \n",
    "    #To avoid division by 0, we put same weight to both predictions, which will just be the mean of the user respectively item, so it makes sense\n",
    "    if no_predictions[0].shape[0] != 0:\n",
    "        items_confidence[no_predictions]=1\n",
    "        users_confidence[no_predictions]=1\n",
    "    \n",
    "    weight_users = (user_weight*users_confidence)/(user_weight*users_confidence + (1-user_weight)*items_confidence)\n",
    "    weight_items = ((1-user_weight)*items_confidence)/(user_weight*users_confidence + (1-user_weight)*items_confidence)\n",
    "\n",
    "    final_predictions = users_pred*weight_users + items_pred*weight_items\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "1ecc6f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.92857143, 4.48859473, 4.83333333, ..., 4.11538462, 5.        ,\n",
       "        5.        ],\n",
       "       [4.72389621, 2.81285549, 4.30962288, ..., 4.36084074, 3.026917  ,\n",
       "        4.58977164],\n",
       "       [2.01847706, 1.02956358, 1.62142332, ..., 2.55555556, 2.09398842,\n",
       "        2.40257751],\n",
       "       ...,\n",
       "       [3.51505848, 2.18800346, 3.01577568, ..., 2.63415318, 3.07987385,\n",
       "        3.03146004],\n",
       "       [3.58266908, 2.22780132, 3.51063618, ..., 2.40858109, 3.06530814,\n",
       "        3.75771876],\n",
       "       [3.15462872, 2.53945976, 3.32887286, ..., 1.37194277, 2.93427735,\n",
       "        4.        ]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_users_and_items(users_pred, items_pred, users_confidence, items_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c1565ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(pred, submission_name=\"submission\", compression_type =\"zip\"):\n",
    "    sample = pd.read_csv('../data/sampleSubmission.csv') # load sample submission\n",
    "    sample = sample.astype({\"Prediction\": float}, errors='raise')\n",
    "    for index, row in sample.iterrows():\n",
    "        r, c = re.findall(r'r(\\d+)_c(\\d+)', row[\"Id\"])[0]\n",
    "        sample.at[index, \"Prediction\"] = pred[int(r)-1][int(c)-1]\n",
    "    \n",
    "    sample.to_csv(submission_name, compression=compression_type, float_format='%.3f', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "585f3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_on_kaggle(name=\"submission.zip\", message=None):\n",
    "    '''\n",
    "    Submit a solution on kaggle.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str (optional)\n",
    "        name of the file to submit\n",
    "    message: str (optional)\n",
    "        Message to use with the submission. Makes easier to \n",
    "        understand what each submission is about\n",
    "    '''\n",
    "    command = f\"kaggle competitions submit -c cil-collaborative-filtering-2022 -f {name}\"\n",
    "\n",
    "    if not message is None:\n",
    "        command = command + f\" -m {message}\"\n",
    "\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "9b8b995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission(pred, submission_name=\"sigmoid_cosine_similarity_items_10nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "7bc620e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_on_kaggle(name=\"sigmoid_cosine_similarity_items_10nn\", message=\"Similarity based using items, cosine similarity with sigmoid weighting and 10 nearest neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "8111e33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.48457718, 3.46575115, 3.29454848, ..., 2.79584825, 2.99129092,\n",
       "        3.33164412],\n",
       "       [3.34920169, 2.47014907, 2.84239086, ..., 5.        , 3.        ,\n",
       "        3.        ],\n",
       "       [2.27405046, 3.02060159, 2.87631228, ..., 2.10074353, 2.37203414,\n",
       "        2.56367779],\n",
       "       ...,\n",
       "       [2.91337448, 3.36292369, 3.19491513, ..., 2.76052163, 2.91109604,\n",
       "        3.4841525 ],\n",
       "       [2.80468284, 3.44195667, 3.54328259, ..., 2.66757643, 2.83965279,\n",
       "        3.04405327],\n",
       "       [2.87667227, 3.44069647, 3.30077702, ..., 2.98914642, 3.08887743,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f4dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
